@inproceedings{allen2018make,
 author = {Zeyuan Allen{-}Zhu},
 booktitle = {Advances in Neural Information Processing Systems 31: Annual Conference
on Neural Information Processing Systems 2018, NeurIPS 2018, December
3-8, 2018, Montr{\'{e}}al, Canada},
 pages = {1165--1175},
 title = {How To Make the Gradients Small Stochastically: Even Faster Convex
and Nonconvex {SGD}},
 year = {2018}
}

@inproceedings{chen2018convergence,
 author = {Xiangyi Chen and
Sijia Liu and
Ruoyu Sun and
Mingyi Hong},
 booktitle = {7th International Conference on Learning Representations, {ICLR} 2019,
New Orleans, LA, USA, May 6-9, 2019},
 title = {On the Convergence of A Class of {A}dam-Type Algorithms for Non-Convex
Optimization},
 year = {2019}
}

@article{defossez2020simple,
 author = {D{\'e}fossez, Alexandre and Bottou, L{\'e}on and Bach, Francis and Usunier, Nicolas},
 journal = {arXiv preprint arXiv:2003.02395},
 title = {A Simple Convergence Proof of {A}dam and {A}dagrad},
 year = {2020}
}

@article{dou2021one,
 author = {Dou, Zehao and Li, Yuanzhi},
 journal = {arXiv preprint arXiv:2109.14213},
 title = {On the One-sided Convergence of {A}dam-type Algorithms in Non-convex Non-concave Min-max Optimization},
 year = {2021}
}

@inproceedings{duchi2011adaptive,
 author = {John C. Duchi and
Elad Hazan and
Yoram Singer},
 booktitle = {{COLT} 2010 - The 23rd Conference on Learning Theory, Haifa, Israel,
June 27-29, 2010},
 pages = {257--269},
 title = {Adaptive Subgradient Methods for Online Learning and Stochastic Optimization},
 year = {2010}
}

@article{ghadimi2016accelerated,
 author = {Ghadimi, Saeed and Lan, Guanghui},
 journal = {Mathematical Programming},
 number = {1-2},
 pages = {59--99},
 title = {Accelerated gradient methods for nonconvex nonlinear and stochastic programming},
 year = {2016}
}

@inproceedings{kingma2014adam,
 author = {Diederik P. Kingma and
Jimmy Ba},
 booktitle = {3rd International Conference on Learning Representations, {ICLR} 2015,
San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings},
 title = {{A}dam: {A} Method for Stochastic Optimization},
 year = {2015}
}

@inproceedings{lu2018accelerating,
 author = {Haihao Lu and
Robert M. Freund and
Vahab S. Mirrokni},
 booktitle = {Proceedings of the 35th International Conference on Machine Learning,
{ICML} 2018, Stockholmsm{\"{a}}ssan, Stockholm, Sweden, July 10-15,
2018},
 pages = {3263--3272},
 series = {Proceedings of Machine Learning Research},
 title = {Accelerating Greedy Coordinate Descent Methods},
 year = {2018}
}

@inproceedings{reddi2018convergence,
 author = {Sashank J. Reddi and
Satyen Kale and
Sanjiv Kumar},
 booktitle = {6th International Conference on Learning Representations, {ICLR} 2018,
Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings},
 title = {On the Convergence of {A}dam and Beyond},
 year = {2018}
}

@article{yang2016unified,
 author = {Yang, Tianbao and Lin, Qihang and Li, Zhe},
 journal = {arXiv preprint arXiv:1604.03257},
 title = {Unified convergence analysis of stochastic momentum methods for convex and non-convex optimization},
 year = {2016}
}

@article{zhou2018convergence,
 author = {Zhou, Dongruo and Chen, Jinghui and Cao, Yuan and Tang, Yiqi and Yang, Ziyan and Gu, Quanquan},
 journal = {arXiv preprint arXiv:1808.05671},
 title = {On the convergence of adaptive gradient methods for nonconvex optimization},
 year = {2018}
}

