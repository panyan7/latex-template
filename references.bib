@article{allen2018make,
  title={How To Make the Gradients Small Stochastically: Even Faster Convex and Nonconvex SGD},
  author={Allen-Zhu, Zeyuan},
  journal={Advances in Neural Information Processing Systems},
  volume={31},
  pages={1157--1167},
  year={2018}
}

@inproceedings{chen2018convergence,
  title={On the Convergence of A Class of Adam-Type Algorithms for Non-Convex Optimization},
  author={Chen, Xiangyi and Liu, Sijia and Sun, Ruoyu and Hong, Mingyi},
  booktitle={International Conference on Learning Representations},
  year={2018}
}

@article{defossez2020simple,
 author = {D{\'e}fossez, Alexandre and Bottou, L{\'e}on and Bach, Francis and Usunier, Nicolas},
 journal = {arXiv preprint arXiv:2003.02395},
 title = {A Simple Convergence Proof of {A}dam and {A}dagrad},
 year = {2020}
}

@article{dou2021one,
 author = {Dou, Zehao and Li, Yuanzhi},
 journal = {arXiv preprint arXiv:2109.14213},
 title = {On the One-sided Convergence of {A}dam-type Algorithms in Non-convex Non-concave Min-max Optimization},
 year = {2021}
}

@article{duchi2011adaptive,
  title={Adaptive subgradient methods for online learning and stochastic optimization.},
  author={Duchi, John and Hazan, Elad and Singer, Yoram},
  journal={Journal of machine learning research},
  volume={12},
  number={7},
  year={2011}
}

@article{ghadimi2016accelerated,
 author = {Ghadimi, Saeed and Lan, Guanghui},
 journal = {Mathematical Programming},
 number = {1-2},
 pages = {59--99},
 title = {Accelerated gradient methods for nonconvex nonlinear and stochastic programming},
 year = {2016}
}

@inproceedings{kingma2014adam,
 author = {Diederik P. Kingma and
Jimmy Ba},
 booktitle = {3rd International Conference on Learning Representations, {ICLR} 2015,
San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings},
 title = {Adam: {A} Method for Stochastic Optimization},
 year = {2015}
}

@inproceedings{lu2018accelerating,
  title={Accelerating greedy coordinate descent methods},
  author={Lu, Haihao and Freund, Robert and Mirrokni, Vahab},
  booktitle={International Conference on Machine Learning},
  pages={3257--3266},
  year={2018},
  organization={PMLR}
}

@inproceedings{reddi2018convergence,
  title={On the Convergence of Adam and Beyond},
  author={Reddi, Sashank J and Kale, Satyen and Kumar, Sanjiv},
  booktitle={International Conference on Learning Representations},
  year={2018}
}

@article{yang2016unified,
 author = {Yang, Tianbao and Lin, Qihang and Li, Zhe},
 journal = {arXiv preprint arXiv:1604.03257},
 title = {Unified convergence analysis of stochastic momentum methods for convex and non-convex optimization},
 year = {2016}
}

@article{zhang2020adaptive,
  title={Why are Adaptive Methods Good for Attention Models?},
  author={Zhang, Jingzhao and Karimireddy, Sai Praneeth and Veit, Andreas and Kim, Seungyeon and Reddi, Sashank and Kumar, Sanjiv and Sra, Suvrit},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  year={2020}
}

@article{zhou2018convergence,
 author = {Zhou, Dongruo and Chen, Jinghui and Cao, Yuan and Tang, Yiqi and Yang, Ziyan and Gu, Quanquan},
 journal = {arXiv preprint arXiv:1808.05671},
 title = {On the convergence of adaptive gradient methods for nonconvex optimization},
 year = {2018}
}

