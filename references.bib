@article{chernoff1952measure,
  title={A measure of asymptotic efficiency for tests of a hypothesis based on the sum of observations},
  author={Chernoff, Herman},
  journal={The Annals of Mathematical Statistics},
  pages={493--507},
  year={1952},
  publisher={JSTOR}
}

@incollection{hoeffding1994probability,
  title={Probability inequalities for sums of bounded random variables},
  author={Hoeffding, Wassily},
  booktitle={The collected works of Wassily Hoeffding},
  pages={409--426},
  year={1994},
  publisher={Springer}
}

@article{robbins1951stochastic,
  title={A stochastic approximation method},
  author={Robbins, Herbert and Monro, Sutton},
  journal={The annals of mathematical statistics},
  pages={400--407},
  year={1951},
  publisher={JSTOR}
}

@article{tropp2012user,
  title={User-friendly tail bounds for sums of random matrices},
  author={Tropp, Joel A},
  journal={Foundations of computational mathematics},
  volume={12},
  number={4},
  pages={389--434},
  year={2012},
  publisher={Springer}
}

@article{tropp2015introduction,
  title={An Introduction to Matrix Concentration Inequalities},
  author={Tropp, Joel A},
  journal={Foundations and Trends{\textregistered} in Machine Learning},
  volume={8},
  number={1-2},
  pages={1--230},
  year={2015},
  publisher={Now Publishers Inc. Hanover, MA, USA}
}

@article{bubeck2015convex,
  title={Convex Optimization: Algorithms and Complexity},
  author={Bubeck, S{\'e}bastien},
  journal={Foundations and Trends{\textregistered} in Machine Learning},
  volume={8},
  number={3-4},
  pages={231--357},
  year={2015},
  publisher={Now Publishers Inc. Hanover, MA, USA}
}

@article{woodruff2014sketching,
  title={Sketching as a Tool for Numerical Linear Algebra},
  author={Woodruff, David P and others},
  journal={Foundations and Trends{\textregistered} in Theoretical Computer Science},
  volume={10},
  number={1--2},
  pages={1--157},
  year={2014},
  publisher={Now Publishers, Inc.}
}

@article{jain2017non,
  title={Non-convex Optimization for Machine Learning},
  author={Jain, Prateek and Kar, Purushottam},
  journal={Foundations and Trends{\textregistered} in Machine Learning},
  volume={10},
  number={3-4},
  pages={142--336},
  year={2017},
  publisher={Now Publishers Inc. Hanover, MA, USA}
}

@book{blum2020foundations,
  title={Foundations of data science},
  author={Blum, Avrim and Hopcroft, John and Kannan, Ravindran},
  year={2020},
  publisher={Cambridge University Press}
}

@book{tao2012topics,
  title={Topics in random matrix theory},
  author={Tao, Terence},
  volume={132},
  year={2012},
  publisher={American Mathematical Soc.}
}

@book{conway2019course,
  title={A course in functional analysis},
  author={Conway, John B},
  volume={96},
  year={2019},
  publisher={Springer}
}

@article{hazan2016introduction,
  title={Introduction to online convex optimization},
  author={Hazan, Elad and others},
  journal={Foundations and Trends{\textregistered} in Optimization},
  volume={2},
  number={3-4},
  pages={157--325},
  year={2016},
  publisher={Now Publishers, Inc.}
}

@inproceedings{clarkson2013low,
  title={Low rank approximation and regression in input sparsity time},
  author={Clarkson, Kenneth L and Woodruff, David P},
  booktitle={Proceedings of the forty-fifth annual ACM symposium on Theory of Computing},
  pages={81--90},
  year={2013}
}

@inproceedings{jacot2018neural,
  title={Neural tangent kernel: Convergence and generalization in neural networks},
  author={Jacot, Arthur and Gabriel, Franck and Hongler, Cl{\'e}ment},
  booktitle={Proceedings of the 32nd International Conference on Neural Information Processing Systems},
  pages={8580--8589},
  year={2018}
}

@article{allen2018convex,
  title={From Convex to Nonconvex Optimization: Algorithm Design and Global Convergence},
  author={Allen-Zhu, Zeyuan},
  year={2018}
}

@article{allen2018make,
  title={How To Make the Gradients Small Stochastically: Even Faster Convex and Nonconvex {SGD}},
  author={Allen-Zhu, Zeyuan},
  journal={Advances in Neural Information Processing Systems},
  volume={31},
  pages={1157--1167},
  year={2018}
}

@inproceedings{bernstein2018signsgd,
  title={{signSGD}: Compressed optimisation for non-convex problems},
  author={Bernstein, Jeremy and Wang, Yu-Xiang and Azizzadenesheli, Kamyar and Anandkumar, Animashree},
  booktitle={International Conference on Machine Learning},
  pages={560--569},
  year={2018},
  organization={PMLR}
}

@inproceedings{chen2018convergence,
  title={On the Convergence of A Class of {Adam}-Type Algorithms for Non-Convex Optimization},
  author={Chen, Xiangyi and Liu, Sijia and Sun, Ruoyu and Hong, Mingyi},
  booktitle={International Conference on Learning Representations},
  year={2018}
}

@article{de2018convergence,
  title={Convergence guarantees for {RMSProp} and {Adam} in non-convex optimization and an empirical comparison to Nesterov acceleration},
  author={De, Soham and Mukherjee, Anirbit and Ullah, Enayat},
  journal={arXiv preprint arXiv:1807.06766},
  year={2018}
}

@article{defossez2020simple,
  title = {A Simple Convergence Proof of {Adam} and {Adagrad}},
  author = {D{\'e}fossez, Alexandre and Bottou, L{\'e}on and Bach, Francis and Usunier, Nicolas},
  journal = {arXiv preprint arXiv:2003.02395},
  year = {2020}
}

@article{dou2021one,
  title = {On the One-sided Convergence of {Adam}-type Algorithms in Non-convex Non-concave Min-max Optimization},
  author = {Dou, Zehao and Li, Yuanzhi},
  journal = {arXiv preprint arXiv:2109.14213},
  year = {2021}
}

@article{duchi2011adaptive,
  title={Adaptive subgradient methods for online learning and stochastic optimization},
  author={Duchi, John and Hazan, Elad and Singer, Yoram},
  journal={Journal of machine learning research},
  volume={12},
  number={7},
  year={2011}
}

@article{fang2019convergence,
  title={Convergence analyses of online {Adam} algorithm in convex setting and two-layer {ReLU} neural network},
  author={Fang, Biyi and Klabjan, Diego},
  journal={arXiv preprint arXiv:1905.09356},
  year={2019}
}

@article{ghadimi2013stochastic,
  title={Stochastic first-and zeroth-order methods for nonconvex stochastic programming},
  author={Ghadimi, Saeed and Lan, Guanghui},
  journal={SIAM Journal on Optimization},
  volume={23},
  number={4},
  pages={2341--2368},
  year={2013},
  publisher={SIAM}
}

@article{ghadimi2016accelerated,
  title={Accelerated gradient methods for nonconvex nonlinear and stochastic programming},
  author={Ghadimi, Saeed and Lan, Guanghui},
  journal={Mathematical Programming},
  volume={156},
  number={1-2},
  pages={59--99},
  year={2016},
  publisher={Springer}
}

@inproceedings{huang2019nostalgic,
  title={Nostalgic {Adam}: Weighting more of the past gradients when designing the adaptive learning rate},
  author={Huang, Haiwen and Wang, Chang and Dong, Bin},
  booktitle={28th International Joint Conference on Artificial Intelligence, IJCAI 2019},
  pages={2556--2562},
  year={2019}
}

@inproceedings{kingma2015adam,
  title={{Adam}: A method for stochastic optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  booktitle={International Conference on Learning Representations},
  year={2015}
}

@inproceedings{li2019convergence,
  title={On the convergence of stochastic gradient descent with adaptive stepsizes},
  author={Li, Xiaoyu and Orabona, Francesco},
  booktitle={The 22nd International Conference on Artificial Intelligence and Statistics},
  pages={983--992},
  year={2019},
  organization={PMLR}
}

@inproceedings{lu2018accelerating,
  title={Accelerating greedy coordinate descent methods},
  author={Lu, Haihao and Freund, Robert and Mirrokni, Vahab},
  booktitle={International Conference on Machine Learning},
  pages={3257--3266},
  year={2018},
  organization={PMLR}
}

@inproceedings{reddi2018convergence,
  title={On the Convergence of {Adam} and Beyond},
  author={Reddi, Sashank J and Kale, Satyen and Kumar, Sanjiv},
  booktitle={International Conference on Learning Representations},
  year={2018}
}

@inproceedings{reddi2018adaptive,
  title={Adaptive methods for nonconvex optimization},
  author={Reddi, S and Zaheer, Manzil and Sachan, Devendra and Kale, Satyen and Kumar, Sanjiv},
  booktitle={Proceeding of 32nd Conference on Neural Information Processing Systems (NIPS 2018)},
  year={2018}
}

@inproceedings{ward2019adagrad,
  title={{AdaGrad} stepsizes: Sharp convergence over nonconvex landscapes},
  author={Ward, Rachel and Wu, Xiaoxia and Bottou, Leon},
  booktitle={International Conference on Machine Learning},
  pages={6677--6686},
  year={2019},
  organization={PMLR}
}

@article{yang2016unified,
 author = {Yang, Tianbao and Lin, Qihang and Li, Zhe},
 journal = {arXiv preprint arXiv:1604.03257},
 title = {Unified convergence analysis of stochastic momentum methods for convex and non-convex optimization},
 year = {2016}
}

@article{zhang2020adaptive,
  title={Why are Adaptive Methods Good for Attention Models?},
  author={Zhang, Jingzhao and Karimireddy, Sai Praneeth and Veit, Andreas and Kim, Seungyeon and Reddi, Sashank and Kumar, Sanjiv and Sra, Suvrit},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  year={2020}
}

@article{zhou2018convergence,
 author = {Zhou, Dongruo and Chen, Jinghui and Cao, Yuan and Tang, Yiqi and Yang, Ziyan and Gu, Quanquan},
 journal = {arXiv preprint arXiv:1808.05671},
 title = {On the convergence of adaptive gradient methods for nonconvex optimization},
 year = {2018}
}

@inproceedings{zou2019sufficient,
  title={A sufficient condition for convergences of {Adam} and {RMSprop}},
  author={Zou, Fangyu and Shen, Li and Jie, Zequn and Zhang, Weizhong and Liu, Wei},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={11127--11135},
  year={2019}
}

@article{zou2018weighted,
  title={Weighted {Adagrad} with unified momentum},
  author={Zou, Fangyu and Shen, Li and Jie, Zequn and Sun, Ju and Liu, Wei},
  journal={arXiv preprint arXiv:1808.03408},
  year={2018}
}

@article{richtarik2014iteration,
  title={Iteration complexity of randomized block-coordinate descent methods for minimizing a composite function},
  author={Richt{\'a}rik, Peter and Tak{\'a}{\v{c}}, Martin},
  journal={Mathematical Programming},
  volume={144},
  number={1},
  pages={1--38},
  year={2014},
  publisher={Springer}
}

@article{song2021training,
  title={Training Multi-Layer Over-Parametrized Neural Network in Subquadratic Time},
  author={Song, Zhao and Zhang, Lichen and Zhang, Ruizhe},
  journal={arXiv preprint arXiv:2112.07628},
  year={2021}
}

@inproceedings{van2021training,
  title={Training (Overparametrized) Neural Networks in Near-Linear Time},
  author={van den Brand, Jan and Peng, Binghui and Song, Zhao and Weinstein, Omri},
  booktitle={12th Innovations in Theoretical Computer Science Conference (ITCS 2021)},
  volume={185},
  pages={63},
  year={2021},
  organization={Schloss Dagstuhl--Leibniz-Zentrum f{\"u}r Informatik}
}

@inproceedings{cohen2020gradient,
  title={Gradient Descent on Neural Networks Typically Occurs at the Edge of Stability},
  author={Cohen, Jeremy and Kaur, Simran and Li, Yuanzhi and Kolter, J Zico and Talwalkar, Ameet},
  booktitle={International Conference on Learning Representations},
  year={2020}
}

@book{martens2016second,
  title={Second-order optimization for neural networks},
  author={Martens, James},
  year={2016},
  publisher={University of Toronto (Canada)}
}

@article{bottou2018optimization,
  title={Optimization methods for large-scale machine learning},
  author={Bottou, L{\'e}on and Curtis, Frank E and Nocedal, Jorge},
  journal={Siam Review},
  volume={60},
  number={2},
  pages={223--311},
  year={2018},
  publisher={SIAM}
}

@inproceedings{jacot2019asymptotic,
  title={The asymptotic spectrum of the Hessian of {DNN} throughout training},
  author={Jacot, Arthur and Gabriel, Franck and Hongler, Clement},
  booktitle={International Conference on Learning Representations},
  year={2019}
}

@article{chen2021visualgpt,
  title={VisualGPT: Data-efficient Adaptation of Pretrained Language Models for Image Captioning},
  author={Chen, Jun and Guo, Han and Yi, Kai and Li, Boyang and Elhoseiny, Mohamed},
  journal={arXiv preprint arXiv:2102.10407},
  year={2021}
}

@article{tsimpoukelli2021multimodal,
  title={Multimodal few-shot learning with frozen language models},
  author={Tsimpoukelli, Maria and Menick, Jacob and Cabi, Serkan and Eslami, SM and Vinyals, Oriol and Hill, Felix},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  year={2021}
}

@inproceedings{du2020few,
  title={Few-Shot Learning via Learning the Representation, Provably},
  author={Du, Simon Shaolei and Hu, Wei and Kakade, Sham M and Lee, Jason D and Lei, Qi},
  booktitle={International Conference on Learning Representations},
  year={2020}
}

@article{bouniot2020towards,
  title={Towards better understanding meta-learning methods through multi-task representation learning theory},
  author={Bouniot, Quentin and Redko, Ievgen and Audigier, Romaric and Loesch, Ang{\'e}lique and Zotkin, Yevhenii and Habrard, Amaury},
  journal={arXiv preprint arXiv:2010.01992},
  year={2020}
}

@inproceedings{tripuraneni2021provable,
  title={Provable meta-learning of linear representations},
  author={Tripuraneni, Nilesh and Jin, Chi and Jordan, Michael},
  booktitle={International Conference on Machine Learning},
  pages={10434--10443},
  year={2021},
  organization={PMLR}
}

@article{arora2017provable,
  title={Provable benefits of representation learning},
  author={Arora, Sanjeev and Risteski, Andrej},
  journal={arXiv preprint arXiv:1706.04601},
  year={2017}
}

@article{rosenfeld2022domain,
  title={Domain-adjusted regression or: {ERM} may already learn features sufficient for out-of-distribution generalization},
  author={Rosenfeld, Elan and Ravikumar, Pradeep and Risteski, Andrej},
  journal={arXiv preprint arXiv:2202.06856},
  year={2022}
}

@article{arora2012multiplicative,
  title={The multiplicative weights update method: A meta-algorithm and applications},
  author={Arora, Sanjeev and Hazan, Elad and Kale, Satyen},
  journal={Theory of computing},
  volume={8},
  number={1},
  pages={121--164},
  year={2012},
  publisher={Theory of Computing Exchange}
}

@article{goodfellow2014generative,
  title={Generative adversarial nets},
  author={Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
  journal={Advances in neural information processing systems},
  volume={27},
  year={2014}
}

@article{cai2019gram,
  title={Gram-gauss-newton method: Learning overparameterized neural networks for regression problems},
  author={Cai, Tianle and Gao, Ruiqi and Hou, Jikai and Chen, Siyu and Wang, Dong and He, Di and Zhang, Zhihua and Wang, Liwei},
  journal={arXiv preprint arXiv:1905.11675},
  year={2019}
}

@article{allen2016lazysvd,
  title={LazySVD: Even faster SVD decomposition yet without agonizing pain},
  author={Allen-Zhu, Zeyuan and Li, Yuanzhi},
  journal={Advances in neural information processing systems},
  volume={29},
  year={2016}
}

@inproceedings{allen2019convergence,
  title={A convergence theory for deep learning via over-parameterization},
  author={Allen-Zhu, Zeyuan and Li, Yuanzhi and Song, Zhao},
  booktitle={International Conference on Machine Learning},
  pages={242--252},
  year={2019},
  organization={PMLR}
}

@article{kawaguchi2016deep,
  title={Deep learning without poor local minima},
  author={Kawaguchi, Kenji},
  journal={Advances in neural information processing systems},
  volume={29},
  year={2016}
}

@inproceedings{du2018gradient,
  title={Gradient Descent Provably Optimizes Over-parameterized Neural Networks},
  author={Du, Simon S and Zhai, Xiyu and Poczos, Barnabas and Singh, Aarti},
  booktitle={International Conference on Learning Representations},
  year={2018}
}

@inproceedings{du2019gradient,
  title={Gradient descent finds global minima of deep neural networks},
  author={Du, Simon and Lee, Jason and Li, Haochuan and Wang, Liwei and Zhai, Xiyu},
  booktitle={International conference on machine learning},
  pages={1675--1685},
  year={2019},
  organization={PMLR}
}

@inproceedings{arora2019fine,
  title={Fine-grained analysis of optimization and generalization for overparameterized two-layer neural networks},
  author={Arora, Sanjeev and Du, Simon and Hu, Wei and Li, Zhiyuan and Wang, Ruosong},
  booktitle={International Conference on Machine Learning},
  pages={322--332},
  year={2019},
  organization={PMLR}
}

@article{arora2019exact,
  title={On exact computation with an infinitely wide neural net},
  author={Arora, Sanjeev and Du, Simon S and Hu, Wei and Li, Zhiyuan and Salakhutdinov, Russ R and Wang, Ruosong},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

@article{allen2019learning,
  title={Learning and generalization in overparameterized neural networks, going beyond two layers},
  author={Allen-Zhu, Zeyuan and Li, Yuanzhi and Liang, Yingyu},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@inproceedings{gutmann2010noise,
  title={Noise-contrastive estimation: A new estimation principle for unnormalized statistical models},
  author={Gutmann, Michael and Hyv{\"a}rinen, Aapo},
  booktitle={Proceedings of the thirteenth international conference on artificial intelligence and statistics},
  pages={297--304},
  year={2010},
  organization={JMLR Workshop and Conference Proceedings}
}

@article{rhodes2020telescoping,
  title={Telescoping density-ratio estimation},
  author={Rhodes, Benjamin and Xu, Kai and Gutmann, Michael U},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={4905--4916},
  year={2020}
}

@article{liu2021analyzing,
  title={Analyzing and Improving the Optimization Landscape of Noise-Contrastive Estimation},
  author={Liu, Bingbin and Rosenfeld, Elan and Ravikumar, Pradeep and Risteski, Andrej},
  journal={arXiv preprint arXiv:2110.11271},
  year={2021}
}

@inproceedings{devlin2019bert,
  title={{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  booktitle={Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},
  pages={4171--4186},
  year={2019}
}

@article{zhang2019gradient,
  title={Why gradient clipping accelerates training: A theoretical justification for adaptivity},
  author={Zhang, Jingzhao and He, Tianxing and Sra, Suvrit and Jadbabaie, Ali},
  journal={arXiv preprint arXiv:1905.11881},
  year={2019}
}

